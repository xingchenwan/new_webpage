<!DOCTYPE html>
<!-- This site was created with Wowchemy. https://www.wowchemy.com -->
<!-- Last Published: October 12, 2024 --><html lang="en-us" >


<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.7.0 for Hugo" />
  

  
  












  
  










  







  
  
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Cutive+Mono&family=Lora:wght@400;700&family=Roboto:wght@400;700&display=swap&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Cutive+Mono&family=Lora:wght@400;700&family=Roboto:wght@400;700&display=swap&display=swap" media="print" onload="this.media='all'">
    
  

  
  

  
  

  

  <link rel="stylesheet" href="/css/vendor-bundle.min.047268c6dd09ad74ba54a0ba71837064.css" media="print" onload="this.media='all'">

  
  
  
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.2/css/academicons.min.css" integrity="sha512-KlJCpRsLf+KKu2VQa5vmRuClRFjxc5lXO03ixZt82HZUk41+1I0bD8KBSA0fY290ayMfWYI9udIqeOWSu1/uZg==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    
    
    
    
      
      
    
    
    

    
    
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      
        
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.cc8a7e0dcd60d69eae9c627013e6a886.css" />

  
  
  

  
  
  
  
  
  
  
    
    
    <link rel="stylesheet" href="/css/libs/chroma/github-light.min.css" title="hl-light" media="print" onload="this.media='all'" >
    <link rel="stylesheet" href="/css/libs/chroma/dracula.min.css" title="hl-dark" media="print" onload="this.media='all'" disabled>
  

  
  






<script async src="https://www.googletagmanager.com/gtag/js?id=G-GHJ9KNNZZB"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
      dataLayer.push(arguments);
  }

  function trackOutboundLink(url, target) {
    gtag('event', 'click', {
         'event_category': 'outbound',
         'event_label': url,
         'transport_type': 'beacon',
         'event_callback': function () {
           if (target !== '_blank') {
             document.location = url;
           }
         }
    });
    console.debug("Outbound link clicked: " + url);
  }

  function onClickCallback(event) {
    if ((event.target.tagName !== 'A') || (event.target.host === window.location.host)) {
      return;
    }
    trackOutboundLink(event.target, event.target.getAttribute('target'));  
  }

  gtag('js', new Date());
  gtag('config', 'G-GHJ9KNNZZB', {});
  gtag('set', {'cookie_flags': 'SameSite=None;Secure'});

  
  document.addEventListener('click', onClickCallback, false);
</script>























  
  
  






  <meta name="author" content="Xingchen Wan" />





  

<meta name="description" content="*Adaptive, black-box and automatic* prompting methods to improve LLMs in zero-shot settings (ACL &amp; EMNLP 2023)" />



<link rel="alternate" hreflang="en-us" href="https://xingchen.one/post/llm_prompting/" />
<link rel="canonical" href="https://xingchen.one/post/llm_prompting/" />





<link rel="icon" type="image/png" href="/media/icon_huab74be30481f90068f99190f5c24f5c6_21326_32x32_fill_lanczos_center_3.png" />
<link rel="apple-touch-icon" type="image/png" href="/media/icon_huab74be30481f90068f99190f5c24f5c6_21326_180x180_fill_lanczos_center_3.png" />

<meta name="theme-color" content="#1565c0" />










  






<meta property="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://xingchen.one/post/llm_prompting/featured.png" />
<meta property="og:site_name" content="Xingchen Wan" />
<meta property="og:url" content="https://xingchen.one/post/llm_prompting/" />
<meta property="og:title" content="Introducing Self-Adaptive Prompting for Large Language Models | Xingchen Wan" />
<meta property="og:description" content="*Adaptive, black-box and automatic* prompting methods to improve LLMs in zero-shot settings (ACL &amp; EMNLP 2023)" /><meta property="og:image" content="https://xingchen.one/post/llm_prompting/featured.png" /><meta property="og:locale" content="en-us" />

  
    <meta
      property="article:published_time"
      content="2023-05-23T00:00:00&#43;00:00"
    />
  
  
    <meta property="article:modified_time" content="2023-10-24T00:00:00&#43;00:00">
  






    






  




<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://xingchen.one/post/llm_prompting/"
  },
  "headline": "Introducing Self-Adaptive Prompting for Large Language Models",
  
  "image": [
    "https://xingchen.one/post/llm_prompting/featured.png"
  ],
  
  "datePublished": "2023-05-23T00:00:00Z",
  "dateModified": "2023-10-24T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "Xingchen Wan"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Xingchen Wan",
    "logo": {
      "@type": "ImageObject",
      "url": "https://xingchen.one/media/icon_huab74be30481f90068f99190f5c24f5c6_21326_192x192_fill_lanczos_center_3.png"
    }
  },
  "description": "*Adaptive, black-box and automatic* prompting methods to improve LLMs in zero-shot settings (ACL \u0026 EMNLP 2023)"
}
</script>

  

  




  
  
  

  
  

  


  
  <title>Introducing Self-Adaptive Prompting for Large Language Models | Xingchen Wan</title>

  
  
  
  











</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="b5e807bc3e364a419038b60da50cb6b8" >

  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.ec9d49ca50e4b80bdb08f0417a28ed84.js"></script>

  




  <div class="page-header header--fixed">
  
  
  
  
  












<header>
  <nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
    <div class="container-xl">

      
      <div class="d-none d-lg-inline-flex">
        <a class="navbar-brand" href="/">Xingchen Wan</a>
      </div>
      

      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
      <span><i class="fas fa-bars"></i></span>
      </button>
      

      
      <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
        <a class="navbar-brand" href="/">Xingchen Wan</a>
      </div>
      

      
      
      <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

        
        <ul class="navbar-nav d-md-inline-flex">
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#about"><span>Home</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#news"><span>News</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#featured"><span>Featured</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#publication"><span>Publications</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#tags"><span>Tags</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#experience"><span>Experience</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#contact"><span>Contact</span></a>
          </li>

          
          

        

          
        </ul>
      </div>

      <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

        
        
          
        

        
        
        

        
        
        
        <li class="nav-item dropdown theme-dropdown">
          <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true" aria-label="Display preferences">
            <i class="fas fa-moon" aria-hidden="true"></i>
          </a>
          <div class="dropdown-menu">
            <a href="#" class="dropdown-item js-set-theme-light">
              <span>Light</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-dark">
              <span>Dark</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-auto">
              <span>Automatic</span>
            </a>
          </div>
        </li>
        

        
        

      </ul>

    </div>
  </nav>
</header>


  </div>

  <div class="page-body">
    
    
    

    <article class="article">

  






















  
  


<div class="article-container pt-3">
  <h1>Introducing Self-Adaptive Prompting for Large Language Models</h1>

  
  <p class="page-subtitle"><em>Adaptive, black-box and automatic</em> prompting methods to improve large language models in zero-shot settings.</p>
  

  


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">
      Xingchen Wan</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
          Last updated on
      
    
    24 Oct 2023
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    8 min read
  </span>
  

  
  
  
  

  
  

</div>

  




<div class="btn-links mb-3">
  
  








  


















  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header" href="https://blog.research.google/2023/11/zero-shot-adaptive-prompting-of-large.html" target="_blank" rel="noopener">
    Google Research Blog</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header" href="https://aclanthology.org/2023.findings-acl.216/" target="_blank" rel="noopener">
    COSP (ACL 2023)</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header" href="https://aclanthology.org/2023.emnlp-main.461/" target="_blank" rel="noopener">
    USP (EMNLP 2023)</a>


</div>


</div>


<div class="article-header container featured-image-wrapper mt-4 mb-4" style="max-width: 699px; max-height: 367px;">
  <div style="position: relative">
    <img src="/post/llm_prompting/featured_huffdcbd3ba80987361ea8427651d594d7_132020_1200x2500_fit_q100_h2_lanczos_3.webp" width="699" height="367" alt="" class="featured-image">
    <span class="article-header-caption">Overall pipeline of COSP/USP</span>
  </div>
</div>



  <div class="article-container">

    <div class="article-style">
      <p><strong>3 Nov 2023: COSP and USP have also been covered in a <a href="https://blog.research.google/2023/11/zero-shot-adaptive-prompting-of-large.html" target="_blank" rel="noopener">recent Google Research blog</a> &ndash; click for more details!</strong></p>
<span style="color:gray">
<i> This post introduces a recent work done during my internship at Google Cloud AI Research covering the following papers:</i>
</span>
<ul>
<li>Xingchen Wan, Ruoxi Sun, Hanjun Dai, Sercan Ö. Arık, Tomas Pfister (2023). <a href="https://aclanthology.org/2023.findings-acl.216/" target="_blank" rel="noopener">Better Zero-Shot Reasoning with Self-Adaptive Prompting</a>. <em>Findings of the Association for Computational Linguistics: ACL 2023</em>.</li>
<li>Xingchen Wan, Ruoxi Sun, Hootan Nakhost, Hanjun Dai, Julian Martin Eisenschlos, Sercan Ö. Arık, Tomas Pfister (2023) <a href="https://aclanthology.org/2023.emnlp-main.461/" target="_blank" rel="noopener">Universal Self-Adaptive Prompting</a>. <em>Empirical Methods in Natural Language Processing (EMNLP)</em>.</li>
</ul>
<p>The recent advances in large language models (LLMs) are among the most astonishing breakthroughs in the history of artificial intelligence. A hallmark feature of the modern LLMs is their impressive abilities in general problem-solving in <em>few-shot</em> and <em>zero-shot</em> setups, even without explicit training on these tasks.
There is, however, still a gap between few-shot and zero-shot setup: in few-shot, the models are shown <em>in-context demonstrations</em> like the example below (taken from the <a href="https://arxiv.org/abs/2201.11903" target="_blank" rel="noopener">chain-of-thought (CoT) paper</a>) &ndash; the first question-answer pair is the demonstration prepended to the actual question being asked (the second question):</p>
<blockquote>
<p><strong>Q:</strong> There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?<br>
<strong>A:</strong> Let&rsquo;s think step by step. There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.<br>
<strong>Q:</strong> If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?<br>
<strong>A:</strong> Let&rsquo;s think step by step.</p>
</blockquote>
<p>In zero-shot, in contrast, the LLM is directly prompted with the <em>test question only</em> (i.e., without the first question-answer pair shown above).</p>
<p>Zero-shot is the most general as it requires no handcrafting and is the most natural way of asking things. However, zero-shot performance is typically weaker as the LLM is not shown with model answers and thus is prone to outputting spurious answers.</p>
<h1 id="why-dont-we-simply-handcraft-some-demos-if-it-is-few-shot">Why don&rsquo;t we simply handcraft some demos if it is &ldquo;few&rdquo;-shot?</h1>
<p>You certainly can &ndash; modern LLMs are great because they can work with a handful of labeled queries (as opposed to hundreds to thousands if you fine-tune them).</p>
<p>However, there are examples where this can be challenging if, for example,</p>
<ol>
<li>You have a lot of tasks, and you end up having to handcraft <code>n_task * n_shot,</code> which can be pretty big even if <code>n_shot</code> is moderate.</li>
<li>Test queries are challenging or time-consuming in the first place (e.g., summarising a long article, answering a medical question that requires expertise or at least some research, or simply the example above: to do chain-of-thought prompting you have to solve a math question by hand first &ndash; quite a challenge for people like me who have graduated from primary school for too long.</li>
</ol>
<h1 id="can-we-have-demonstrations-even-in-zero-shot">Can we have demonstrations even in zero-shot?</h1>
<p>We know that LLMs benefit from demonstrations (otherwise, few-shot won&rsquo;t be better!), and we know that LLMs have at least <em>some</em> zero-shot abilities. So the natural next step is&hellip;</p>
<h3 id="why-not-use-the-models-own-outputs-as-demonstrations">Why not use the model&rsquo;s own outputs as demonstrations?</h3>
<p>Several previous works have proposed this, like <a href="https://arxiv.org/abs/2210.03493" target="_blank" rel="noopener">AutoCoT</a>: We ask the LLMs to give answers under zero-shot prompting and prompt the models again with their outputs as demonstrations. A problem, though, is precisely what we mentioned just now &ndash; <em>zero-shot solutions are imperfect</em>, and we risk giving LLMs wrong demonstrations, which in some cases can be worse than <em>no demonstrations at all</em>. So the question now is&hellip;</p>
<h3 id="can-we-select-_good_-outputs-as-demonstrations-without-verifying-their-correctness">Can we select <em>good</em> outputs as demonstrations without verifying their correctness?</h3>
<p>A trivial way is to sift through the zero-shot outputs and retain correct answers. This, however, substitutes the manual effort in hand-labelling demos in the few-shot setup with hand-verifying outputs in zero-shot answers and defeats our purpose of achieving automatic prompting.</p>
<p>This is where <em>self-adaptive prompting</em> comes in: the TL;DR of the key idea is:</p>
<p><span style="color:blue"><strong>Confident and consistent answers from the LLMs are more likely correct.</strong></span></p>
<p>This, of course, depends on how good the uncertainty estimate the LLMs have, but in large models, both previous works like <a href="https://arxiv.org/abs/2207.05221" target="_blank" rel="noopener">this</a> and <a href="https://arxiv.org/abs/2210.11610" target="_blank" rel="noopener">this</a> and our results have shown that they are fairly well-calibrated.</p>
<p>To measure confidence, we borrow the <a href="https://arxiv.org/abs/2203.11171" target="_blank" rel="noopener">self-consistency</a> idea (but not just for majority vote): In <a href="https://aclanthology.org/2023.findings-acl.216/" target="_blank" rel="noopener">Consistency-Based Self-Adaptive Prompting (COSP)</a>, our ACL 2023 paper, we ask the same question multiple times but with a non-zero temperature to induce stochasticity: if the model is certain, it should output the same answer each time and vice versa. We then compute the <em>entropy</em> of the answers to gauge the uncertainty.</p>
<p>We later generalize this simple idea in <em>Universal</em> Self-Adaptive Prompting to two additional setups for general NLP tasks beyond reasoning:</p>
<ol>
<li><em>Classification</em> (CLS), where we have the output <em>logits</em> &ndash; we can measure the uncertainty there without multiple sampling</li>
<li><em>Short</em>-form generation (SFG), like question answering, we can use the same procedure mentioned above for COSP, but without the rationale-generating step.</li>
<li><em>Long</em> text generation (LFG), like summarization and translation, the questions are often open-ended. The outputs are unlikely to be identical verbatim even if the LLM is certain, given the generation length and the fact that multiple answers can be equally plausible, we use an overlap metric instead by computing the average of the pairwise ROUGE score between the different outputs to the same query.</li>
</ol>
<p>We compute the relevant confidence scores depending on the type of task on the aforementioned set of unlabeled test samples. After scoring, we pick the confident outputs, plus some diversity encouragement and repetition penalty, to form a model-generated pseudo-demonstration set. We finally query the LLM again in a few-shot format with these pseudo-demonstrations to obtain the final predictions on the entire test set.</p>
<p>Crucially, in all cases, this only requires unlabeled data and LLM outputs but no ground-truth labels at any point in time, and thus, the entire approach is zero-shot, or more precisely <a href="https://arxiv.org/abs/1703.04394" target="_blank" rel="noopener">transductive zero-shot</a>. There are several additional things to the idea above, and please check the papers for the details.</p>
<h3 id="putting-all-together">Putting all together</h3>
<p>















<figure  id="figure-illustration-of-usp-in-exemplary-tasks-classification-qa-and-text-summarization-similar-to-cosp-the-llm-first-generates-predictions-on-an-unlabeled-dataset-whose-outputs-are-scored-with-logit-entropy-consistency-or-alignment-depending-on-the-task-type-and-pseudo-demonstrations-are-selected-from-these-input-output-pairs-in-stage-2-the-test-instances-are-augmented-with-pseudo-demos-for-prediction">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt="Illustration of USP in exemplary tasks (classification, QA and text summarization). Similar to COSP, the LLM first generates predictions on an unlabeled dataset whose outputs are scored with logit entropy, consistency or alignment, depending on the task type, and pseudo-demonstrations are selected from these input-output pairs. In Stage 2, the test instances are augmented with pseudo-demos for prediction."
           src="/post/llm_prompting/usp_animation.gif"
           loading="lazy" data-zoomable /></div>
  </div><figcaption>
      Illustration of USP in exemplary tasks (classification, QA and text summarization). Similar to COSP, the LLM first generates predictions on an unlabeled dataset whose outputs are scored with logit entropy, consistency or alignment, depending on the task type, and pseudo-demonstrations are selected from these input-output pairs. In Stage 2, the test instances are augmented with pseudo-demos for prediction.
    </figcaption></figure>
</p>
<p>Putting all together, we have the following pipeline,</p>
<ol>
<li>Run zero-shot prompting to obtain a bunch of outputs on some test outputs, using self-consistency as mentioned above if necessary.</li>
<li>Use the procedure described to <em>score</em> these outputs, and select the confident outputs as demos (plus some diversity encouragement and/or repetition penalty).</li>
<li>Using the outputs obtained in Step 2 as <em>pseudo</em>-demos, query the LLM again in a few-shot format to get the final predictions.</li>
</ol>
<p>&hellip; which is what we show in the cover picture of this post.</p>
<h1 id="results">Results</h1>
<p>On COSP, which focuses on reasoning tasks:</p>
<p>















<figure  id="figure-comparison-of-zero-shot-cosp-and-standard-zero-shot-and-few-shot-setups-averaged-across-six-reasoning-tasks">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt="Comparison of zero-shot COSP and standard zero-shot and few-shot setups, averaged across six reasoning tasks." srcset="
               /post/llm_prompting/cosp_tweet_1_huef8c3067217d4cd586cc4cc51234a6b0_22244_3cfe191328c8dfe74f4faadc2c46dcac.webp 400w,
               /post/llm_prompting/cosp_tweet_1_huef8c3067217d4cd586cc4cc51234a6b0_22244_6c358fa9d43372efb1eeaf3db2827559.webp 760w,
               /post/llm_prompting/cosp_tweet_1_huef8c3067217d4cd586cc4cc51234a6b0_22244_1200x1200_fit_q100_h2_lanczos_3.webp 1200w"
               src="/post/llm_prompting/cosp_tweet_1_huef8c3067217d4cd586cc4cc51234a6b0_22244_3cfe191328c8dfe74f4faadc2c46dcac.webp"
               width="400"
               height="369"
               loading="lazy" data-zoomable /></div>
  </div><figcaption>
      Comparison of zero-shot COSP and standard zero-shot and few-shot setups, averaged across six reasoning tasks.
    </figcaption></figure>
</p>
<p>We can see that zero-shot COSP massively outperforms standard zero-shot (<a href="https://arxiv.org/abs/2205.11916" target="_blank" rel="noopener"><em>Let&rsquo;s think step by step</em></a> only) and is on par or better than 5-shot with labeled examples over 3 LLMs. We also outperform previous SoTAs like AutoCoT (shown in the paper). See the papers for more results and analyses!</p>
<p>In USP, we expand our analysis to a much wider range of tasks, including more than 25 classifications, short-form generation, and long-form generation tasks. We also study the <a href="https://arxiv.org/abs/2210.09261" target="_blank" rel="noopener">BIG-Bench Hard</a> suite of tasks where LLMs previously underperformed humans using the state-of-the-art <a href="https://blog.google/technology/ai/google-palm-2-ai-large-language-model/" target="_blank" rel="noopener">PaLM 2</a> models. We show that in all cases, USP again outperforms the baselines and is competitive to prompting with golden examples.</p>
<p>















<figure  id="figure-key-results-of-usp-cls-refers-to-an-average-of-15-classification-tasks-sfg-refers-to-an-average-of-5-short-form-generation-tasks-lfg-refers-to-an-average-of-2-summarization-tasks">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt="Key results of USP: CLS refers to an average of 15 classification tasks; SFG refers to an average of 5 short-form generation tasks; LFG refers to an average of 2 summarization tasks." srcset="
               /post/llm_prompting/usp_results_hu6b6d67c2eaa63bac42484b93bc31badf_281495_74ca759df3a6db5943943c30f8ce4c5a.webp 400w,
               /post/llm_prompting/usp_results_hu6b6d67c2eaa63bac42484b93bc31badf_281495_5ed475fdd88dab8af2387f5bfec7f0f6.webp 760w,
               /post/llm_prompting/usp_results_hu6b6d67c2eaa63bac42484b93bc31badf_281495_1200x1200_fit_q100_h2_lanczos_3.webp 1200w"
               src="/post/llm_prompting/usp_results_hu6b6d67c2eaa63bac42484b93bc31badf_281495_74ca759df3a6db5943943c30f8ce4c5a.webp"
               width="760"
               height="385"
               loading="lazy" data-zoomable /></div>
  </div><figcaption>
      Key results of USP: CLS refers to an average of 15 classification tasks; SFG refers to an average of 5 short-form generation tasks; LFG refers to an average of 2 summarization tasks.
    </figcaption></figure>
</p>
<p>We also analyze the working mechanism of USP by validating the key observation above on the relation between confidence and correctness, and we found that in an overwhelming majority of the cases, USP picks confident predictions that are more likely better in all task types considered, as shown in the figure below.</p>
<p>















<figure  id="figure-usp-picks-confident-predictions-that-are-more-likely-better-ground-truth-performance-metrics-against-usp-confidence-scores-in-selected-tasks-in-various-task-types-blue-cls-orange-sfg-green-lfg-with-palm-540b">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt="USP picks confident predictions that are more likely better. Ground-truth performance metrics against USP confidence scores in selected tasks in various task types (blue: CLS, orange: SFG, green: LFG) with PaLM-540B." srcset="
               /post/llm_prompting/usp_mechanism_hude2f0a1709fd6f561dae3a22100aab44_135191_c1250642023cb2eacbb983c54d7a015b.webp 400w,
               /post/llm_prompting/usp_mechanism_hude2f0a1709fd6f561dae3a22100aab44_135191_2c3dd850e9362b4a85388a1f186eb899.webp 760w,
               /post/llm_prompting/usp_mechanism_hude2f0a1709fd6f561dae3a22100aab44_135191_1200x1200_fit_q100_h2_lanczos_3.webp 1200w"
               src="/post/llm_prompting/usp_mechanism_hude2f0a1709fd6f561dae3a22100aab44_135191_c1250642023cb2eacbb983c54d7a015b.webp"
               width="760"
               height="170"
               loading="lazy" data-zoomable /></div>
  </div><figcaption>
      USP picks confident predictions that are more likely better. Ground-truth performance metrics against USP confidence scores in selected tasks in various task types (blue: CLS, orange: SFG, green: LFG) with PaLM-540B.
    </figcaption></figure>
</p>
<h1 id="conclusion">Conclusion</h1>
<p>Zero-shot inference is a highly sought-after capability of modern LLMs, yet the success in which poses unique challenges. We propose COSP and USP, a family of versatile, zero-shot automatic prompting techniques applicable to a wide range of tasks. We show large improvement over the state-of-the-art baselines over numerous task and model combinations.</p>
<h1 id="acknowledgements">Acknowledgements</h1>
<p><em>This work was conducted by Xingchen Wan, Ruoxi Sun, Hootan Nakhost, Hanjun Dai, Julian Martin Eisenschlos, Sercan Ö. Arık, and Tomas Pfister. We would like to thank Jinsung Yoon and Xuezhi Wang for providing helpful reviews, and other colleagues at Google Cloud AI Research for their discussion and feedback.</em></p>

    </div>

    





<div class="article-tags">
  
  <a class="badge badge-light" href="/tag/large-language-models/">Large Language Models</a>
  
</div>



<div class="share-box">
  <ul class="share">
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https%3A%2F%2Fxingchen.one%2Fpost%2Fllm_prompting%2F&amp;text=Introducing&#43;Self-Adaptive&#43;Prompting&#43;for&#43;Large&#43;Language&#43;Models" target="_blank" rel="noopener" class="share-btn-twitter" aria-label="twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https%3A%2F%2Fxingchen.one%2Fpost%2Fllm_prompting%2F&amp;t=Introducing&#43;Self-Adaptive&#43;Prompting&#43;for&#43;Large&#43;Language&#43;Models" target="_blank" rel="noopener" class="share-btn-facebook" aria-label="facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
        
      
      <li>
        <a href="mailto:?subject=Introducing%20Self-Adaptive%20Prompting%20for%20Large%20Language%20Models&amp;body=https%3A%2F%2Fxingchen.one%2Fpost%2Fllm_prompting%2F" target="_blank" rel="noopener" class="share-btn-email" aria-label="envelope">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https%3A%2F%2Fxingchen.one%2Fpost%2Fllm_prompting%2F&amp;title=Introducing&#43;Self-Adaptive&#43;Prompting&#43;for&#43;Large&#43;Language&#43;Models" target="_blank" rel="noopener" class="share-btn-linkedin" aria-label="linkedin-in">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="whatsapp://send?text=Introducing&#43;Self-Adaptive&#43;Prompting&#43;for&#43;Large&#43;Language&#43;Models%20https%3A%2F%2Fxingchen.one%2Fpost%2Fllm_prompting%2F" target="_blank" rel="noopener" class="share-btn-whatsapp" aria-label="whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https%3A%2F%2Fxingchen.one%2Fpost%2Fllm_prompting%2F&amp;title=Introducing&#43;Self-Adaptive&#43;Prompting&#43;for&#43;Large&#43;Language&#43;Models" target="_blank" rel="noopener" class="share-btn-weibo" aria-label="weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>











  
  
    



  
  
  
    
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <a href="https://xingchen.one/"><img class="avatar mr-3 avatar-circle" src="/authors/admin/avatar_hu533d715e0ed39c14466a055fb3a2396a_6688507_270x270_fill_q100_lanczos_center.JPG" alt="Xingchen Wan"></a>
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://xingchen.one/">Xingchen Wan</a></h5>
      <h6 class="card-subtitle">Research Scientist</h6>
      <p class="card-text">My research interests include large language models, Bayesian optimization, AutoML, and machine learning on graphs.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="/#contact" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/xingchenwan/" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.com/citations?user=6KkohssAAAAJ" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/xingchenwan" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://research.google/people/xingchenwan/" target="_blank" rel="noopener">
        <i class="fab fa-google"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>


  
















  </div>
</article>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  












  
  
  
  
  













  
  
  

  
  
    
  
  
    
  

  

  
  <p class="powered-by copyright-license-text">
    © 2019 - 2024 Xingchen Wan. This work is licensed under <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank">CC BY NC ND 4.0</a>
  </p>
  

  <p class="powered-by footer-license-icons">
    <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank" aria-label="Creative Commons">
      <i class="fab fa-creative-commons fa-2x" aria-hidden="true"></i>
      <i class="fab fa-creative-commons-by fa-2x" aria-hidden="true"></i>
      
        <i class="fab fa-creative-commons-nc fa-2x" aria-hidden="true"></i>
      
      
        <i class="fab fa-creative-commons-nd fa-2x" aria-hidden="true"></i>
      
    </a>
  </p>





  <p class="powered-by">
    
    
    
      
      
      
      
      
      
      Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Wowchemy</a> — the free, <a href="https://github.com/wowchemy/wowchemy-hugo-themes" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
    
  </p>
</footer>

    </div>
    
  </div>

  


<script src="/js/vendor-bundle.min.1d4346c6f7d46c340dc0a9058dd85c13.js"></script>




  

  
  

  






























<script id="page-data" type="application/json">{"use_headroom":true}</script>



  <script src="/js/wowchemy-headroom.db4755770454eb63685f8de785c0a172.js" type="module"></script>










<script src="/en/js/wowchemy.min.94ff8956498596ae447982c55e230726.js"></script>







  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        
        <pre><code></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>


  <script src="/js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js" type="module"></script>


















</body>
</html>
