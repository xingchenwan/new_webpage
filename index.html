<!DOCTYPE html>
<html lang="en">
  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <!-- Metadata, OpenGraph and Schema.org -->




<!-- Standard metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<title>
  
  
    Xingchen Wan
  
</title>
<meta name="author" content="Xingchen Wan">
<meta name="description" content="">










<!-- Bootstrap & MDB -->
<link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">



<!-- Fonts & Icons -->
<link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5">
<link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772">
<link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap">

<!-- Code Syntax Highlighting -->
<link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light">



<!-- Styles -->




  <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8C%9F&lt;/text&gt;&lt;/svg&gt;">

<link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
<link rel="canonical" href="https://xingchen.one/">


  <!-- Dark Mode -->
  <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script>
  <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark">
  <script>
    initTheme();
  </script>










  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">
    <!-- Header -->
    <header>
  <!-- Nav Bar -->
  <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation">
    <div class="container">
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>

      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          

          <!-- About -->
          <li class="nav-item active">
            <a class="nav-link" href="/">about
              
                <span class="sr-only">(current)</span>
              
            </a>
          </li>

          <!-- Other pages -->
          
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
              
                
                <li class="nav-item ">
                  <a class="nav-link" href="/publications/">publications
                    
                  </a>
                </li>
              
            
          
          
            <!-- Search -->
            <li class="nav-item">
              <button id="search-toggle" title="Search" onclick="openSearchModal()">
                <span class="nav-link">ctrl k <i class="ti ti-search"></i></span>
              </button>
            </li>
          
          
            <!-- Toogle theme mode -->
            <li class="toggle-container">
              <button id="light-toggle" title="Change theme">
                <i class="ti ti-sun-moon" id="light-toggle-system"></i>
                <i class="ti ti-moon-filled" id="light-toggle-dark"></i>
                <i class="ti ti-sun-filled" id="light-toggle-light"></i>
              </button>
            </li>
          
        </ul>
      </div>
    </div>
  </nav>
  
    <!-- Scrolling Progress Bar -->
    <progress id="progress" value="0">
      <div class="progress-container">
        <span class="progress-bar"></span>
      </div>
    </progress>
  
</header>


    <!-- Content -->
    <div class="container mt-5" role="main">
      
        <div class="post">
  <header class="post-header">
    <h1 class="post-title">
      
        <span class="font-weight-bold">Xingchen</span> 
        Wan
      
    </h1>
    <p class="desc">Senior Research Scientist, <a href="https://www.google.com" rel="external nofollow noopener" target="_blank">Google</a></p>
  </header>

  <article>
    
      <div class="profile float-right">
        
          
          
          
          



<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      
        <source class="responsive-img-srcset" srcset="/assets/img/prof_pic.JPG" sizes="(min-width: 930px) 270.0px, (min-width: 576px)
      30vw, 95vw">
      
    
    <img src="/assets/img/prof_pic.JPG?8fc075c3c1d86e20115dd62e6c823391" class="img-fluid z-depth-1
      rounded" width="100%" height="auto" alt="prof_pic.JPG" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </source></picture>

  
</figure>

        
        
          <div class="more-info">
<p>1600 Amphitheatre Parkway</p> <p>Mountain View, CA 94043</p> <p>xingchenw[at]google.com</p>
</div>
        
      </div>
    

    <div class="clearfix">
<p>I am a <strong>Senior Research Scientist at <a href="https://research.google/" rel="external nofollow noopener" target="_blank"><img src="/assets/img/google_logo.png" style="height: 18px;"></a></strong> based in the San Francisco Bay Area.</p>

<h3 id="research-interests">Research Interests</h3>

<p>My primary research drives innovations in <strong>large language models (LLMs)</strong>, focusing on building systems that are more efficient, robust, and autonomous. My contributions span:</p>

<ul>
  <li>
<strong>LLM Post-training</strong> (e.g., [<a href="https://arxiv.org/pdf/2505.11409" rel="external nofollow noopener" target="_blank">1</a>, <a href="https://openreview.net/pdf?id=HbwkIDWQgN" rel="external nofollow noopener" target="_blank">2</a>, <a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00662/120914" rel="external nofollow noopener" target="_blank">3</a>, <a href="https://arxiv.org/abs/2512.01945" rel="external nofollow noopener" target="_blank">4</a>]);</li>
  <li>Developing <strong>self-improving (multimodal) LLM agents</strong> (e.g., [<a href="https://arxiv.org/pdf/2510.15831?" rel="external nofollow noopener" target="_blank">5</a>, <a href="https://arxiv.org/pdf/2509.10704" rel="external nofollow noopener" target="_blank">6</a>, <a href="https://arxiv.org/pdf/2502.00330" rel="external nofollow noopener" target="_blank">7</a>]);</li>
  <li>
<strong>Automating prompt and agent design</strong> (e.g., [<a href="https://arxiv.org/pdf/2406.15708" rel="external nofollow noopener" target="_blank">8</a>, <a href="https://arxiv.org/pdf/2502.02533" rel="external nofollow noopener" target="_blank">9</a>, <a href="https://arxiv.org/pdf/2406.11370?" rel="external nofollow noopener" target="_blank">10</a>]); and</li>
  <li>Integrating <strong>GenAI with large-scale (unstructured) data systems</strong> (e.g., [<a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/34b3a40ec9752c1ae48fe85fef8fe8dc-Paper-Conference.pdf" rel="external nofollow noopener" target="_blank">11</a>, <a href="https://aclanthology.org/2025.acl-long.1476.pdf" rel="external nofollow noopener" target="_blank">12</a>]).</li>
</ul>

<p>Previously, I did my PhD in the <a href="http://www.robots.ox.ac.uk/~parg/" rel="external nofollow noopener" target="_blank">Machine Learning Research Group</a>, <a href="http://www.eng.ox.ac.uk/" rel="external nofollow noopener" target="_blank">Department of Engineering Science</a>, <a href="https://ox.ac.uk/" rel="external nofollow noopener" target="_blank">University of Oxford</a> where I worked on Bayesian optimization, AutoML, and machine learning on graphs.</p>

<h3 id="academic-services">Academic Services</h3>
<p><strong>Area chair/senior program committee member</strong> at ICML (2025, 2026), NeurIPS (2024, 2025), ACL ARR (2025-); Action editor at TMLR.</p>

<p><strong>Reviewer/program committee member</strong> at ACL (2023-24), AutoML-Conf (2023-24), COLM (2024), CVPR (2024), ECCV (2024), EMNLP (2023-24), ICLR (2024-25), ICML (2023-24), JMLR, Machine Learning, NeurIPS (2022-23), WACV (2022-24), etc.</p>
</div>

    <!-- News -->
    
      <h2>
        <a href="/news/" style="color: inherit">news</a>
      </h2>
      <div class="news">
  
    
    <div class="table-responsive">
      <table class="table table-sm table-borderless">
        
        
        
          <tr>
            <th scope="row" style="width: 20%">Feb 05, 2026</th>
            <td>
              
                Two papers have been accepted to <strong>ICLR 2026</strong>: <a href="https://arxiv.org/pdf/2505.11409" rel="external nofollow noopener" target="_blank">Visual Planning with Reinforcement Learning</a> (<strong>Oral</strong>) and <a href="https://arxiv.org/pdf/2502.02533" rel="external nofollow noopener" target="_blank">MASS</a> (Optimizing Agents with Better Prompts and Topologies).

              
            </td>
          </tr>
        
          <tr>
            <th scope="row" style="width: 20%">Oct 17, 2025</th>
            <td>
              
                We present <a href="https://arxiv.org/pdf/2510.15831" rel="external nofollow noopener" target="_blank">VISTA</a> and <a href="https://arxiv.org/pdf/2509.10704" rel="external nofollow noopener" target="_blank">Maestro</a>, two self-improving multimodal generation agents for text-to-video and text-to-image generation, respectively.

<blockquote class="twitter-tweet" width="500"><p lang="en" dir="ltr">üö® Google just dropped the most advanced self-improving video AI ever built.<br><br>It‚Äôs called VISTA, and it literally rewrites its own prompts to make every new generation better than the last.<br><br>No retraining. No fine-tuning. Just pure test-time self-reflection.<br><br>Here‚Äôs how it works:‚Ä¶ <a href="https://t.co/WyhX9uur9l" rel="external nofollow noopener" target="_blank">pic.twitter.com/WyhX9uur9l</a>‚Äî Louis Gleeson (@aigleeson) <a href="https://twitter.com/aigleeson/status/1981666197952405838?ref_src=twsrc%5Etfw" rel="external nofollow noopener" target="_blank">October 24, 2025</a></p></blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>


              
            </td>
          </tr>
        
          <tr>
            <th scope="row" style="width: 20%">May 19, 2025</th>
            <td>
              
                We present <a href="https://arxiv.org/pdf/2505.11409" rel="external nofollow noopener" target="_blank">Visual Planning</a>, where we apply reinforcement learning post-training on <em>pure-vision</em> models to achieve state-of-the-art performance in visual reasoning tasks.

<blockquote class="twitter-tweet" width="500"><p lang="en" dir="ltr">üöÄLet‚Äôs Think Only with Images.<br><br>No language and No verbal thought.ü§î <br><br>Let‚Äôs think through a sequence of imagesüí≠, like how humans picture steps in their mindsüé®. <br><br>We propose Visual Planning, a novel reasoning paradigm that enables models to reason purely through images. <a href="https://t.co/ly9JtuEC33" rel="external nofollow noopener" target="_blank">pic.twitter.com/ly9JtuEC33</a>‚Äî Yi Xu (@_yixu) <a href="https://twitter.com/_yixu/status/1924497238908375072?ref_src=twsrc%5Etfw" rel="external nofollow noopener" target="_blank">May 19, 2025</a></p></blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<a href="https://github.com/yix8/VisualPlanning" rel="external nofollow noopener" target="_blank"><img src="https://gh-card.dev/repos/yix8/VisualPlanning.svg"></a>

              
            </td>
          </tr>
        
      </table>
    </div>
  
</div>

    

    <!-- Latest posts -->
    

    <!-- Selected papers -->
    
      <h2>
        <a href="/publications/" style="color: inherit">selected publications</a>
      </h2>
      <div class="publications">
  <ol class="bibliography">
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100" style="background-color:#00369f">
            
              <a href="https://arXiv.org" rel="external nofollow noopener" target="_blank">arXiv</a>
            
          </abbr>
        
      
      
        
          
          



<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      
    
    <img src="/assets/img/publication_preview/vista_spaceship.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="vista_spaceship.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

        
      
    </div>
  

  <!-- Entry bib key -->
  <div id="long2025vista" class="col-sm-8">
    <!-- Title -->
    <div class="title">VISTA: A Test-Time Self-Improving Video Generation Agent</div>
    <!-- Author -->
    <div class="author">
      

      
      Do Xuan
            Long, <em>Xingchen
            Wan</em>, Hootan
            Nakhost, Chen-Yu
            Lee, Tomas
            Pfister, and Sercan √ñ.
            Arƒ±k
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>arXiv preprint arXiv:2510.15831</em>,  2025
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
        <a href="https://doi.org/10.48550/arxiv.2510.15831" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a>
      
      
      
      
      
        
          <a href="https://arxiv.org/pdf/2510.15831" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
      
      
      
        <a href="https://g-vista.github.io/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a>
      
    </div>
    
      
      

      
      

      
      

      
      
      
        <div class="badges">
          
            <span class="altmetric-embed" data-badge-type="2" data-badge-popover="right" data-arxiv-id="2510.15831"></span>
          
          
          
            <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=6KkohssAAAAJ&amp;citation_for_view=6KkohssAAAAJ:738O_yMBCRsC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank">
              <img src="https://img.shields.io/badge/scholar-2-4285F4?logo=googlescholar&amp;labelColor=beige" alt="2 Google Scholar citations">
            </a>
          
          
        </div>
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Despite rapid advances in text-to-video synthesis, generated video quality remains critically dependent on precise user prompts. Existing test-time optimization methods, successful in other domains, struggle with the multi-faceted nature of video. In this work, we introduce VISTA (Video Iterative Self-improvemenT Agent), a novel multi-agent system that autonomously improves video generation through refining prompts in an iterative loop. VISTA first decomposes a user idea into a structured temporal plan. After generation, the best video is identified through a robust pairwise tournament. This winning video is then critiqued by a trio of specialized agents focusing on visual, audio, and contextual fidelity. Finally, a reasoning agent synthesizes this feedback to introspectively rewrite and enhance the prompt for the next generation cycle. Experiments on single- and multi-scene video generation scenarios show that while prior methods yield inconsistent gains, VISTA consistently improves video quality and alignment with user intent, achieving up to 60% pairwise win rate against state-of-the-art baselines. Human evaluators concur, preferring VISTA outputs in 66.4% of comparisons.</p>
      </div>
    

    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100" style="background-color:#00369f">
            
              <a href="https://arXiv.org" rel="external nofollow noopener" target="_blank">arXiv</a>
            
          </abbr>
        
      
      
        
          
          



<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      
    
    <img src="/assets/img/publication_preview/maestro.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="maestro.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

        
      
    </div>
  

  <!-- Entry bib key -->
  <div id="wan2025maestro" class="col-sm-8">
    <!-- Title -->
    <div class="title">Maestro: Self-Improving Text-to-Image Generation via Agent Orchestration</div>
    <!-- Author -->
    <div class="author">
      

      
      <em>Xingchen
            Wan</em>, Han
            Zhou, Ruoxi
            Sun, Hootan
            Nakhost, Ke
            Jiang, Rajarishi
            Sinha, and Sercan √ñ.
            Arƒ±k
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>arXiv preprint arXiv:2509.10704</em>,  2025
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
      
      
      
        
          <a href="https://arxiv.org/pdf/2509.10704" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
      
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
        <div class="badges">
          
          
          
            <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=6KkohssAAAAJ&amp;citation_for_view=6KkohssAAAAJ:XiSMed-E-HIC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank">
              <img src="https://img.shields.io/badge/scholar-4-4285F4?logo=googlescholar&amp;labelColor=beige" alt="4 Google Scholar citations">
            </a>
          
          
        </div>
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Text-to-image (T2I) models, while offering immense creative potential, are highly reliant on human intervention, posing significant usability challenges that often necessitate manual, iterative prompt engineering over often underspecified prompts. This paper introduces Maestro, a novel self-evolving image generation system that enables T2I models to autonomously self-improve generated images through iterative evolution of prompts, using only an initial prompt. Maestro incorporates two key innovations: 1) self-critique, where specialized multimodal LLM (MLLM) agents act as ‚Äôcritics‚Äô to identify weaknesses in generated images, correct for under-specification, and provide interpretable edit signals, which are then integrated by a ‚Äôverifier‚Äô agent while preserving user intent; and 2) self-evolution, utilizing MLLM-as-a-judge for head-to-head comparisons between iteratively generated images, eschewing problematic images, and evolving creative prompt candidates that align with user intents. Extensive experiments on complex T2I tasks using black-box models demonstrate that Maestro significantly improves image quality over initial prompts and state-of-the-art automated methods, with effectiveness scaling with more advanced MLLM components. This work presents a robust, interpretable, and effective pathway towards self-improving T2I generation.</p>
      </div>
    

    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100">ICLR 2026</abbr>
        
      
      
        
          
          



<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      
    
    <img src="/assets/img/publication_preview/vprl.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="vprl.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

        
      
    </div>
  

  <!-- Entry bib key -->
  <div id="xu2025visualplanningletsthink" class="col-sm-8">
    <!-- Title -->
    <div class="title">Visual Planning: Let‚Äôs Think Only with Images</div>
    <!-- Author -->
    <div class="author">
      

      
      Yi
            Xu<sup>*</sup>, Chengzu
            Li<sup>*</sup>, Han
            Zhou<sup>*</sup>, <em>Xingchen
            Wan</em>, Caiqi
            Zhang, Anna
            Korhonen, and Ivan
            Vuliƒá
      
        <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="* equal contribution">
        </i>
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>The Fourteenth International Conference on Learning Representations (to appear)</em>.  üèÜü•â <strong>#3 paper of the day at <a href="https://huggingface.co/papers/2505.11409" rel="external nofollow noopener" target="_blank">HuggingFace ü§ó</a></strong>
,  2026
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
      
      
      
        
          <a href="https://arxiv.org/pdf/2505.11409" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
        <a href="https://github.com/yix8/VisualPlanning" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      
      
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
        <div class="badges">
          
          
          
            <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=6KkohssAAAAJ&amp;citation_for_view=6KkohssAAAAJ:dshw04ExmUIC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank">
              <img src="https://img.shields.io/badge/scholar-37-4285F4?logo=googlescholar&amp;labelColor=beige" alt="37 Google Scholar citations">
            </a>
          
          
        </div>
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Recent advancements in Large Language Models (LLMs) and their multimodal extensions (MLLMs) have substantially enhanced machine reasoning across diverse tasks. However, these models predominantly rely on pure text as the medium for both expressing and structuring reasoning, even when visual information is present. In this work, we argue that language may not always be the most natural or effective modality for reasoning, particularly in tasks involving spatial and geometrical information. Motivated by this, we propose a new paradigm, Visual Planning, which enables planning through purely visual representations, independent of text. In this paradigm, planning is executed via sequences of images that encode step-by-step inference in the visual domain, akin to how humans sketch or visualize future actions. We introduce a novel reinforcement learning framework, Visual Planning via Reinforcement Learning (VPRL), empowered by GRPO for post-training large vision models, leading to substantial improvements in planning in a selection of representative visual navigation tasks, FrozenLake, Maze, and MiniBehavior. Our visual planning paradigm outperforms all other planning variants that conduct reasoning in the text-only space. Our results establish Visual Planning as a viable and promising alternative to language-based reasoning, opening new avenues for tasks that benefit from intuitive, image-based inference.</p>
      </div>
    

    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100">ICLR 2026</abbr>
        
      
      
        
          
          



<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      
    
    <img src="/assets/img/publication_preview/mass.jpeg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="mass.jpeg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

        
      
    </div>
  

  <!-- Entry bib key -->
  <div id="zhou2025multiagentdesignoptimizingagents" class="col-sm-8">
    <!-- Title -->
    <div class="title">Multi-Agent Design: Optimizing Agents with Better Prompts and Topologies</div>
    <!-- Author -->
    <div class="author">
      

      
      Han
            Zhou, <em>Xingchen
            Wan</em>, Ruoxi
            Sun, Hamid
            Palangi, Shariq
            Iqbal, Ivan
            Vuliƒá, Anna
            Korhonen, and Sercan √ñ.
            Arƒ±k
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>The Fourteenth International Conference on Learning Representations (to appear)</em>,  2026
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
        <a href="https://doi.org/10.48550/arxiv.2502.02533" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a>
      
      
      
      
      
        
          <a href="https://arxiv.org/pdf/2502.02533" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
      
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
        <div class="badges">
          
            <span class="altmetric-embed" data-badge-type="2" data-badge-popover="right" data-arxiv-id="2502.02533"></span>
          
          
          
            <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=6KkohssAAAAJ&amp;citation_for_view=6KkohssAAAAJ:b0M2c_1WBrUC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank">
              <img src="https://img.shields.io/badge/scholar-67-4285F4?logo=googlescholar&amp;labelColor=beige" alt="67 Google Scholar citations">
            </a>
          
          
        </div>
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Large language models, employed as multiple agents that interact and collaborate with each other, have excelled at solving complex tasks. The agents are programmed with prompts that declare their functionality, along with the topologies that orchestrate interactions across agents. Designing prompts and topologies for multi-agent systems (MAS) is inherently complex. To automate the entire design process, we first conduct an in-depth analysis of the design space aiming to understand the factors behind building effective MAS. We reveal that prompts together with topologies play critical roles in enabling more effective MAS design. Based on the insights, we propose Multi-Agent System Search (MASS), a MAS optimization framework that efficiently exploits the complex MAS design space by interleaving its optimization stages, from local to global, from prompts to topologies, over three stages: 1) block-level (local) prompt optimization; 2) workflow topology optimization; 3) workflow-level (global) prompt optimization, where each stage is conditioned on the iteratively optimized prompts/topologies from former stages. We show that MASS-optimized multi-agent systems outperform a spectrum of existing alternatives by a substantial margin. Based on the MASS-found systems, we finally propose design principles behind building effective multi-agent systems.</p>
      </div>
    

    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100">ICLR 2025</abbr>
        
      
      
        
          
          



<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      
    
    <img src="/assets/img/publication_preview/bridge.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="bridge.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

        
      
    </div>
  

  <!-- Entry bib key -->
  <div id="wan2025selfimproving" class="col-sm-8">
    <!-- Title -->
    <div class="title">From Few to Many: Self-Improving Many-Shot Reasoners Through Iterative Optimization and Generation</div>
    <!-- Author -->
    <div class="author">
      

      
      <em>Xingchen
            Wan</em>, Han
            Zhou, Ruoxi
            Sun, Hootan
            Nakhost, Ke
            Jiang, and Sercan √ñ.
            Arƒ±k
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In The Thirteenth International Conference on Learning Representations</em>,  2025
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
      
      
      
        
          <a href="https://arxiv.org/pdf/2502.00330" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
      
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
        <div class="badges">
          
          
          
            <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=6KkohssAAAAJ&amp;citation_for_view=6KkohssAAAAJ:EUQCXRtRnyEC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank">
              <img src="https://img.shields.io/badge/scholar-12-4285F4?logo=googlescholar&amp;labelColor=beige" alt="12 Google Scholar citations">
            </a>
          
          
        </div>
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Recent advances in long-context large language models (LLMs) have led to the emerging paradigm of many-shot in-context learning (ICL), where it is observed that scaling many more demonstrating examples beyond the conventional few-shot setup in the context can lead to performance benefits. However, despite its promise, it is unclear what aspects dominate the benefits and whether simply scaling to more examples is the most effective way of improving many-shot ICL. In this work, we first provide an analysis of the factors driving many-shot ICL, and we find that 1) many-shot performance can still be attributed to often a few disproportionately influential examples and 2) identifying such influential examples ("optimize") and using them as demonstrations to regenerate new examples ("generate") can lead to further improvements. Inspired by the findings, we propose BRIDGE, an algorithm that alternates between the optimize step with Bayesian optimization to discover the influential sets of examples and the generate step to reuse this set to expand the reasoning paths of the examples back to the many-shot regime automatically. On Gemini, Claude, and Mistral LLMs of different sizes, we show that BRIDGE to significant improvements across a diverse set of tasks, including symbolic reasoning, numerical reasoning, and code generation.</p>
      </div>
    

    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100">ACL 2025</abbr>
        
      
      
        
          
          



<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      
    
    <img src="/assets/img/publication_preview/astuterag.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="astuterag.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

        
      
    </div>
  

  <!-- Entry bib key -->
  <div id="wang-etal-2025-astute" class="col-sm-8">
    <!-- Title -->
    <div class="title">Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models</div>
    <!-- Author -->
    <div class="author">
      

      
      Fei
            Wang, <em>Xingchen
            Wan</em>, Ruoxi
            Sun, Jiefeng
            Chen, and Sercan O
            Arik
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>,  2025
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
        <a href="https://doi.org/10.48550/arxiv.2410.07176" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a>
      
      
      
      
      
        
          <a href="https://aclanthology.org/2025.acl-long.1476.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
      
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
        <div class="badges">
          
            <span class="altmetric-embed" data-badge-type="2" data-badge-popover="right" data-doi="10.48550/arxiv.2410.07176"></span>
          
          
          
            <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=6KkohssAAAAJ&amp;citation_for_view=6KkohssAAAAJ:a0OBvERweLwC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank">
              <img src="https://img.shields.io/badge/scholar-100-4285F4?logo=googlescholar&amp;labelColor=beige" alt="100 Google Scholar citations">
            </a>
          
          
        </div>
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Retrieval augmented generation (RAG), while effectively integrating external knowledge to address the inherent limitations of large language models (LLMs), can be hindered by imperfect retrieval that contain irrelevant, misleading, or even malicious information. Previous studies have rarely connected the behavior of RAG through joint analysis, particularly regarding error propagation coming from imperfect retrieval and potential conflicts between LLMs‚Äô internal knowledge and external sources. Through comprehensive and controlled analyses under realistic conditions, we find that imperfect retrieval augmentation is inevitable, common, and harmful. We identify the knowledge conflicts between LLM-internal and external knowledge from retrieval as a bottleneck to overcome imperfect retrieval in the post-retrieval stage of RAG. To address this, we propose Astute RAG, a novel RAG approach designed to be resilient to imperfect retrieval augmentation. It adaptively elicits essential information from LLMs‚Äô internal knowledge, iteratively consolidates internal and external knowledge with source-awareness, and finalizes the answer according to information reliability. Our experiments with Gemini and Claude demonstrate the superior performance of Astute RAG compared to previous robustness-enhanced RAG approaches. Specifically, Astute RAG is the only RAG method that achieves performance comparable to or even surpassing conventional use of LLMs under the worst-case scenario. Further analysis reveals the effectiveness of Astute RAG in resolving knowledge conflicts, thereby improving the trustworthiness of RAG.</p>
      </div>
    

    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100">NeurIPS 2024</abbr>
        
      
      
        
          
          



<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      
    
    <img src="/assets/img/publication_preview/ioes.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="ioes.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

        
      
    </div>
  

  <!-- Entry bib key -->
  <div id="Wan2024TeachBetter" class="col-sm-8">
    <!-- Title -->
    <div class="title">Teach Better or Show Smarter? On Instructions and Exemplars in Automatic Prompt Optimization</div>
    <!-- Author -->
    <div class="author">
      

      
      <em>Xingchen
            Wan</em>, Ruoxi
            Sun, Hootan
            Nakhost, and Sercan √ñ.
            Arik
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In Advances in Neural Information Processing Systems 37</em>. ‚òÅÔ∏è <strong>Powers the <a href="https://cloud.google.com/blog/products/ai-machine-learning/announcing-vertex-ai-prompt-optimizer" rel="external nofollow noopener" target="_blank">Google Cloud Vertex AI Prompt Optimizer</a></strong>
,  2024
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
      
      
      
        
          <a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/6b031defd145b02bed031093d8797bb3-Paper-Conference.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
      
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
        <div class="badges">
          
          
          
            <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=6KkohssAAAAJ&amp;citation_for_view=6KkohssAAAAJ:u_35RYKgDlwC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank">
              <img src="https://img.shields.io/badge/scholar-30-4285F4?logo=googlescholar&amp;labelColor=beige" alt="30 Google Scholar citations">
            </a>
          
          
        </div>
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Large language models have demonstrated remarkable capabilities but their performance is heavily reliant on effective prompt engineering. Automatic prompt optimization (APO) methods are designed to automate this and can be broadly categorized into those targeting instructions (instruction optimization, IO) vs. those targeting exemplars (exemplar optimization, EO). Despite their shared objective, these have evolved rather independently, with IO receiving more research attention recently. This paper seeks to bridge this gap by comprehensively comparing the performance of representative IO and EO techniques both isolation and combination on a diverse set of challenging tasks. Our findings reveal that intelligently reusing model-generated input-output pairs obtained from evaluating prompts on the validation set as exemplars, consistently improves performance on top of IO methods but is currently under-investigated. We also find that despite the recent focus on IO, how we select exemplars can outweigh how we optimize instructions, with EO strategies as simple as random search outperforming state-of-the-art IO methods with seed instructions without any optimization. Moreover, we observe a synergy between EO and IO, with optimal combinations surpassing the individual contributions. We conclude that studying exemplar optimization both as a standalone method and its optimal combination with instruction optimization remain a crucial aspect of APO and deserve greater consideration in future research, even in the era of highly capable instruction-following models.</p>
      </div>
    

    

    
  </div>
</div>
</li>
</ol>
</div>

    

    <!-- Social -->
    
      <div class="social">
        <div class="contact-icons">
  
      <a href="https://github.com/xingchenwan" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a>
    

  
      <a href="https://www.linkedin.com/in/xingchenwan" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a>
    

  
      <a href="https://scholar.google.com/citations?user=6KkohssAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a>
    

  
      <a href="https://twitter.com/wanxingchen_" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a>
    

</div>

        <div class="contact-note"></div>
      </div>
    

    
  </article>
</div>

      
    </div>

    <!-- Footer -->
    


  <footer class="fixed-bottom" role="contentinfo">
    <div class="container mt-0">
      
  ¬© Copyright 2026
  Xingchen
  
  Wan. 
  
  

    </div>
  </footer>



    <!-- JavaScripts -->
    <!-- jQuery -->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

<!-- Bootsrap & MDB scripts -->
<script src="/assets/js/bootstrap.bundle.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>


  <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script>























  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script>






<!-- Load Common JS -->
<script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script>
<script defer src="/assets/js/common.js?c15de51d4bb57887caa2c21988d97279"></script>
<script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script>

<!-- Jupyter Open External Links New Tab -->
<script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script>

<!-- Badges -->

  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>


  <script async src="https://badge.dimensions.ai/badge.js"></script>



  <!-- MathJax -->
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script>
  
    <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script>
    <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script>
  



  <!-- Analytics -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-GHJ9KNNZZB"></script>
  <script defer src="/assets/js/google-analytics-setup.js"></script>







  <!-- Scrolling Progress Bar -->
  <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script>







  <!-- Back to Top -->
  <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script>
  <script>
    addBackToTop();
  </script>



  <!-- Search -->
  <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script>
  <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys>
  <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script>
  <script src="/assets/js/search-data.js"></script>
  <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script>




  </body>
</html>
